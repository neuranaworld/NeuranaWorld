"""
LLM Router: AkÄ±llÄ± model seÃ§imi ve ensemble protokolÃ¼
"""
import asyncio
import hashlib
import json
from typing import Dict, Any, List, Tuple
from datetime import datetime
from emergentintegrations.llm.chat import LlmChat, UserMessage
import os
from dotenv import load_dotenv

load_dotenv()

EMERGENT_LLM_KEY = os.getenv("EMERGENT_LLM_KEY")

class LLMRouter:
    """Ã‡oklu-LLM yÃ¶nlendirme ve ensemble sistemi"""
    
    def __init__(self):
        self.cache = {}
        
    async def route_request(
        self, 
        question_type: str, 
        content: str, 
        mode: str = "auto",
        user_preference: str = "auto"
    ) -> Dict[str, Any]:
        """
        Soru tipine gÃ¶re uygun modeli seÃ§ ve yanÄ±t al
        
        Args:
            question_type: "math_deep", "math_fast", "turkish_grammar", "translate"
            content: Soru iÃ§eriÄŸi
            mode: "deep" (ensemble) veya "fast" (tek model)
            user_preference: "auto", "gpt-4o", "claude-sonnet-4", "gemini-2.0-flash"
        """
        
        # KullanÄ±cÄ± tercihi varsa onu kullan
        if user_preference != "auto":
            return await self._call_single_model(user_preference, content)
        
        # AkÄ±llÄ± yÃ¶nlendirme
        if question_type == "math_deep" and mode == "deep":
            return await self._math_deep_think(content)
        elif question_type == "math_fast":
            return await self._call_single_model("gemini-2.0-flash", content)
        elif question_type == "turkish_grammar":
            return await self._turkish_grammar(content)
        elif question_type == "translate":
            return await self._ensemble_translate(content)
        else:
            # Default: GPT-4o
            return await self._call_single_model("gpt-4o", content)
    
    async def _math_deep_think(self, question: str) -> Dict[str, Any]:
        """
        Matematik derin dÃ¼ÅŸÃ¼nme modu
        - Primary: GPT-4o
        - Verify: Claude
        - Fallback: Gemini
        """
        try:
            # Primary: GPT-4o
            primary_result = await self._call_single_model(
                "gpt-4o", 
                question,
                system_message="Sen matematiksel problemleri adÄ±m adÄ±m Ã§Ã¶zen bir uzmansÄ±n. Her adÄ±mÄ± aÃ§Ä±kla ve sonuca ulaÅŸ. Emin deÄŸilsen belirt."
            )
            
            # GÃ¼ven dÃ¼ÅŸÃ¼kse veya verification gerekiyorsa
            if primary_result.get("confidence", 1.0) < 0.9:
                # Claude ile doÄŸrula
                verify_prompt = f"""
AÅŸaÄŸÄ±daki matematiksel Ã§Ã¶zÃ¼mÃ¼ doÄŸrula:

Soru: {question}

Ã‡Ã¶zÃ¼m: {primary_result.get('answer', '')}

Bu Ã§Ã¶zÃ¼m doÄŸru mu? EÄŸer hata varsa dÃ¼zelt ve aÃ§Ä±kla.
"""
                verify_result = await self._call_single_model(
                    "claude-sonnet-4",
                    verify_prompt,
                    system_message="Sen matematiksel Ã§Ã¶zÃ¼mleri doÄŸrulayan bir uzmansÄ±n."
                )
                
                return {
                    "answer": verify_result.get("answer"),
                    "confidence": 0.95,
                    "model": "gpt-4o-verified-by-claude",
                    "verification": verify_result.get("answer"),
                    "original": primary_result.get("answer")
                }
            
            return primary_result
            
        except Exception as e:
            # Fallback: Gemini
            print(f"GPT-4o/Claude failed, falling back to Gemini: {e}")
            return await self._call_single_model("gemini-2.0-flash", question)
    
    async def _turkish_grammar(self, question: str) -> Dict[str, Any]:
        """
        TÃ¼rkÃ§e dilbilgisi sorularÄ±
        - Primary: Claude (TÃ¼rkÃ§e gÃ¼Ã§lÃ¼)
        - Verify: GPT-4o
        """
        try:
            primary_result = await self._call_single_model(
                "claude-sonnet-4",
                question,
                system_message="Sen TÃ¼rkÃ§e dilbilgisi uzmanÄ±sÄ±n. Noktalama, fiilimsi, tamlamalar konusunda detaylÄ± aÃ§Ä±klama yap."
            )
            
            return primary_result
            
        except Exception as e:
            print(f"Claude failed, falling back to GPT-4o: {e}")
            return await self._call_single_model("gpt-4o", question)
    
    async def _ensemble_translate(self, content: str) -> Dict[str, Any]:
        """
        Ã‡eviri iÃ§in ensemble (GPT-4o + Claude)
        Ä°ki modelin Ã§evirilerini karÅŸÄ±laÅŸtÄ±r
        """
        try:
            # Parse content
            data = json.loads(content) if isinstance(content, str) else content
            text = data.get("text", "")
            source_lang = data.get("source_lang", "auto")
            target_lang = data.get("target_lang", "en")
            
            # Her iki modelden Ã§eviri al
            gpt_result = await self._call_single_model(
                "gpt-4o",
                f"Ã‡evir: '{text}' [{source_lang} -> {target_lang}]. Sadece Ã§eviriyi yaz, baÅŸka aÃ§Ä±klama yapma.",
                system_message="Sen profesyonel bir Ã§evirmenisin."
            )
            
            claude_result = await self._call_single_model(
                "claude-sonnet-4",
                f"Ã‡evir: '{text}' [{source_lang} -> {target_lang}]. Sadece Ã§eviriyi yaz, baÅŸka aÃ§Ä±klama yapma.",
                system_message="Sen profesyonel bir Ã§evirmenisin."
            )
            
            gpt_translation = gpt_result.get("answer", "")
            claude_translation = claude_result.get("answer", "")
            
            # Ã‡eviriler benzer mi?
            similarity = self._calculate_similarity(gpt_translation, claude_translation)
            
            if similarity > 0.8:
                # Benzer, yÃ¼ksek gÃ¼ven
                return {
                    "answer": gpt_translation,
                    "confidence": 0.95,
                    "model": "gpt-4o-claude-ensemble",
                    "alternative": claude_translation,
                    "similarity": similarity
                }
            else:
                # FarklÄ± Ã§eviriler, dÃ¼ÅŸÃ¼k gÃ¼ven
                return {
                    "answer": gpt_translation,
                    "confidence": 0.7,
                    "model": "gpt-4o-claude-ensemble",
                    "alternative": claude_translation,
                    "similarity": similarity,
                    "warning": "Ä°ki model farklÄ± Ã§eviriler Ã¼ret ti. Ä°kisini de gÃ¶zden geÃ§irin."
                }
                
        except Exception as e:
            print(f"Ensemble translation failed: {e}")
            return await self._call_single_model("gpt-4o", content)
    
    async def _call_single_model(
        self, 
        model: str, 
        content: str,
        system_message: str = "Sen yardÄ±mcÄ± bir asistansÄ±n."
    ) -> Dict[str, Any]:
        """Tek bir modelden yanÄ±t al"""
        
        # Cache kontrolÃ¼
        cache_key = hashlib.md5(f"{model}:{content}".encode()).hexdigest()
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            # Model ve provider mapping
            provider, model_name = self._get_provider_and_model(model)
            
            # LLM Chat oluÅŸtur
            chat = LlmChat(
                api_key=EMERGENT_LLM_KEY,
                session_id=f"session-{cache_key[:8]}",
                system_message=system_message
            ).with_model(provider, model_name)
            
            # Mesaj gÃ¶nder
            user_message = UserMessage(text=content)
            response = await chat.send_message(user_message)
            
            result = {
                "answer": response,
                "confidence": 1.0,
                "model": model,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            # Cache'e ekle
            self.cache[cache_key] = result
            
            return result
            
        except Exception as e:
            print(f"Error calling {model}: {e}")
            raise
    
    def _get_provider_and_model(self, model: str) -> Tuple[str, str]:
        """Model string'inden provider ve model name Ã§Ä±kar"""
        mapping = {
            "gpt-4o": ("openai", "gpt-4o"),
            "gpt-4o-mini": ("openai", "gpt-4o-mini"),
            "claude-sonnet-4": ("anthropic", "claude-3-7-sonnet-20250219"),
            "gemini-2.0-flash": ("gemini", "gemini-2.0-flash"),
        }
        return mapping.get(model, ("openai", "gpt-4o"))
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Ä°ki metin arasÄ±nda basit benzerlik skoru (0-1)"""
        if not text1 or not text2:
            return 0.0
        
        # Basit token-based similarity
        tokens1 = set(text1.lower().split())
        tokens2 = set(text2.lower().split())
        
        if not tokens1 or not tokens2:
            return 0.0
        
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        return len(intersection) / len(union) if union else 0.0
    
    async def k_self_consistency(
        self, 
        model: str, 
        question: str, 
        k: int = 3
    ) -> Dict[str, Any]:
        """
        K-self-consistency: AynÄ± soruyu farklÄ± sÄ±caklÄ±klarla k kez sor
        Ã‡oÄŸunluk oyuyla sonuÃ§ belirle
        """
        temperatures = [0.3, 0.7, 1.0][:k]
        results = []
        
        for temp in temperatures:
            # TODO: Temperature parametresi ekle
            result = await self._call_single_model(model, question)
            results.append(result.get("answer", ""))
        
        # Ã‡oÄŸunluk oyu (basit versiyon)
        from collections import Counter
        answer_counts = Counter(results)
        most_common = answer_counts.most_common(1)[0]
        
        confidence = most_common[1] / k  # KaÃ§ tanesinde aynÄ± cevap
        
        return {
            "answer": most_common[0],
            "confidence": confidence,
            "model": f"{model}-k{k}-consistency",
            "all_answers": results
        }


    async def multi_ai_consensus(
        self,
        question: str,
        max_iterations: int = 3
    ) -> Dict[str, Any]:
        """
        3 AI konsensus sistemi
        - Gemini, ChatGPT, Claude'a aynÄ± soruyu sor
        - FarklÄ± cevaplar gelirse, tekrar sor ve karÅŸÄ±laÅŸtÄ±r
        - Konsensusa ulaÅŸana kadar devam et (max 3 iterasyon)
        """
        
        models = ["gemini-2.0-flash", "gpt-4o", "claude-sonnet-4"]
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            print(f"ğŸ”„ Konsensus iterasyonu {iteration}/{max_iterations}")
            
            # Her 3 modelden yanÄ±t al (paralel)
            tasks = [
                self._call_single_model(model, question)
                for model in models
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # HatalarÄ± filtrele
            valid_results = [
                r for r in results 
                if not isinstance(r, Exception) and r.get("answer")
            ]
            
            if len(valid_results) < 2:
                return {
                    "answer": "Yeterli AI yanÄ±tÄ± alÄ±namadÄ±",
                    "confidence": 0.0,
                    "model": "multi-ai-consensus-failed",
                    "error": "Insufficient responses"
                }
            
            # CevaplarÄ± karÅŸÄ±laÅŸtÄ±r
            answers = [r["answer"] for r in valid_results]
            answer_models = [models[i] for i, r in enumerate(results) if not isinstance(r, Exception)]
            
            # AynÄ± cevap var mÄ± kontrol et
            consensus_answer, consensus_count = self._find_consensus(answers)
            
            if consensus_count >= 2:  # En az 2 AI aynÄ± cevabÄ± verdi
                confidence = consensus_count / len(valid_results)
                
                return {
                    "answer": consensus_answer,
                    "confidence": confidence,
                    "model": "multi-ai-consensus",
                    "consensus_count": f"{consensus_count}/{len(valid_results)}",
                    "iteration": iteration,
                    "all_answers": [
                        {"model": answer_models[i], "answer": ans}
                        for i, ans in enumerate(answers)
                    ]
                }
            
            # Konsensus yok, tekrar dene
            print(f"âš ï¸ Konsensus bulunamadÄ±, tekrar soruyorum...")
            print(f"Cevaplar: {answers}")
            
            # EÄŸer son iterasyonsa, Ã§oÄŸunluk oylamasÄ± yap
            if iteration == max_iterations:
                # En Ã§ok tekrar eden cevabÄ± bul
                from collections import Counter
                answer_counts = Counter(answers)
                most_common_answer, count = answer_counts.most_common(1)[0]
                
                return {
                    "answer": most_common_answer,
                    "confidence": count / len(answers),
                    "model": "multi-ai-majority-vote",
                    "consensus_count": f"{count}/{len(answers)}",
                    "iteration": iteration,
                    "all_answers": [
                        {"model": answer_models[i], "answer": ans}
                        for i, ans in enumerate(answers)
                    ],
                    "warning": "Tam konsensus saÄŸlanamadÄ±, Ã§oÄŸunluk kararÄ± alÄ±ndÄ±"
                }
        
        return {
            "answer": "Konsensus saÄŸlanamadÄ±",
            "confidence": 0.0,
            "model": "multi-ai-consensus-failed"
        }
    
    def _find_consensus(self, answers: List[str]) -> Tuple[str, int]:
        """
        Cevaplar arasÄ±nda konsensus bul
        Benzer cevaplarÄ± grupla (normalizasyon ile)
        """
        from collections import Counter
        
        # CevaplarÄ± normalize et (kÃ¼Ã§Ã¼k harf, trim, sayÄ±larÄ± ayÄ±kla)
        normalized = []
        for ans in answers:
            # SayÄ±larÄ± bul
            import re
            numbers = re.findall(r'-?\d+\.?\d*', ans)
            if numbers:
                # SayÄ±sal cevap
                normalized.append(numbers[0])
            else:
                # Metin cevap
                normalized.append(ans.strip().lower()[:100])
        
        # En Ã§ok tekrar edeni bul
        counter = Counter(normalized)
        if not counter:
            return answers[0], 1
        
        most_common_normalized, count = counter.most_common(1)[0]
        
        # Orijinal cevabÄ± bul
        for i, norm in enumerate(normalized):
            if norm == most_common_normalized:
                return answers[i], count
        
        return answers[0], 1
    
    async def generate_and_validate_question(
        self,
        question_type: str,
        difficulty: str = "medium"
    ) -> Dict[str, Any]:
        """
        Soru oluÅŸtur ve 3 AI ile doÄŸrula
        
        Args:
            question_type: "grammar", "math", "pattern", "word_game"
            difficulty: "easy", "medium", "hard", "very_hard"
        """
        
        # 1. Gemini ile soru oluÅŸtur
        generator_prompt = self._get_generator_prompt(question_type, difficulty)
        
        gemini_result = await self._call_single_model(
            "gemini-2.0-flash",
            generator_prompt,
            system_message="Sen eÄŸitim sorularÄ± oluÅŸturan bir uzmansÄ±n. JSON formatÄ±nda yanÄ±t ver."
        )
        
        generated_question = gemini_result.get("answer", "")
        
        # 2. ChatGPT ve Claude ile doÄŸrula
        validation_prompt = f"""
AÅŸaÄŸÄ±daki soruyu kontrol et:

{generated_question}

Soru doÄŸru mu? MantÄ±k hatasÄ±, yazÄ±m hatasÄ± veya belirsizlik var mÄ±?
Sadece "DOÄRU" veya hatalarÄ± listele.
"""
        
        gpt_validation = await self._call_single_model("gpt-4o", validation_prompt)
        claude_validation = await self._call_single_model("claude-sonnet-4", validation_prompt)
        
        gpt_valid = "doÄŸru" in gpt_validation.get("answer", "").lower()
        claude_valid = "doÄŸru" in claude_validation.get("answer", "").lower()
        
        if gpt_valid and claude_valid:
            return {
                "question": generated_question,
                "validated": True,
                "confidence": 1.0,
                "validators": ["gpt-4o", "claude-sonnet-4"]
            }
        else:
            return {
                "question": generated_question,
                "validated": False,
                "confidence": 0.5,
                "gpt_feedback": gpt_validation.get("answer"),
                "claude_feedback": claude_validation.get("answer"),
                "warning": "Soru validasyondan geÃ§emedi"
            }
    
    def _get_generator_prompt(self, question_type: str, difficulty: str) -> str:
        """Soru tipi iÃ§in prompt oluÅŸtur"""
        
        prompts = {
            "grammar": f"""
TÃ¼rkÃ§e dilbilgisi sorusu oluÅŸtur (Zorluk: {difficulty}).
Konu: BÃ¼yÃ¼k harflerin kullanÄ±mÄ± (A harfi deÄŸil, genel bÃ¼yÃ¼k harf kurallarÄ±).

JSON formatÄ±nda yanÄ±t ver:
{{
    "question": "Soru metni",
    "options": ["A", "B", "C", "D"],
    "correct_answer": "A",
    "explanation": "AÃ§Ä±klama"
}}
""",
            "word_game": f"""
Adam asmaca iÃ§in kelime ve ipucu oluÅŸtur (Zorluk: {difficulty}).

JSON formatÄ±nda yanÄ±t ver:
{{
    "word": "KELIME",
    "hint": "Ä°pucu metni",
    "category": "Kategori"
}}
""",
            "pattern": f"""
Ã–rÃ¼ntÃ¼ bulma sorusu oluÅŸtur (Zorluk: {difficulty}).

JSON formatÄ±nda yanÄ±t ver:
{{
    "pattern": [1, 2, 4, 8, "?"],
    "answer": 16,
    "explanation": "Her sayÄ± 2 ile Ã§arpÄ±lÄ±yor"
}}
""",
            "math": f"""
Matematik sorusu oluÅŸtur (Zorluk: {difficulty}).

JSON formatÄ±nda yanÄ±t ver:
{{
    "question": "Soru metni",
    "answer": "Cevap",
    "steps": ["AdÄ±m 1", "AdÄ±m 2"]
}}
"""
        }
        
        return prompts.get(question_type, prompts["math"])

# Global router instance
router = LLMRouter()
